<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>SIESTA-QETSc</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2017-02-16T13:36:33-06:00</updated><entry><title>Sample input file</title><link href="/sample-input-file.html" rel="alternate"></link><published>2017-02-16T13:36:33-06:00</published><updated>2017-02-16T13:36:33-06:00</updated><author><name>Murat Keceli</name></author><id>tag:None,2017-02-16:/sample-input-file.html</id><summary type="html">&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# Main options&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# NumberOfAtoms 2040&lt;/span&gt;
&lt;span class="cp"&gt;# NumberOfSpecies 5&lt;/span&gt;
&lt;span class="n"&gt;xc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;functional&lt;/span&gt; &lt;span class="n"&gt;GGA&lt;/span&gt;
&lt;span class="n"&gt;xc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;authors&lt;/span&gt; &lt;span class="n"&gt;PBE&lt;/span&gt;
&lt;span class="n"&gt;PAO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasisSize&lt;/span&gt; &lt;span class="n"&gt;SZ&lt;/span&gt;
&lt;span class="n"&gt;MeshCutoff&lt;/span&gt; &lt;span class="mf"&gt;300.0&lt;/span&gt; &lt;span class="n"&gt;Ry&lt;/span&gt;
&lt;span class="n"&gt;SystemName&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
&lt;span class="n"&gt;SystemLabel&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
&lt;span class="n"&gt;UseSaveData&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;SolutionMethod&lt;/span&gt;  &lt;span class="n"&gt;qetsc&lt;/span&gt; &lt;span class="cp"&gt;#slepc0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;older&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;
&lt;span class="cp"&gt;#BlockSize 23 #Do not set for qetsc&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# System info&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;ChemicalSpeciesLabel&lt;/span&gt;
  &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;  &lt;span class="n"&gt;C&lt;/span&gt;
  &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="n"&gt;H&lt;/span&gt;
  &lt;span class="mi"&gt;3 â€¦&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# Main options&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# NumberOfAtoms 2040&lt;/span&gt;
&lt;span class="cp"&gt;# NumberOfSpecies 5&lt;/span&gt;
&lt;span class="n"&gt;xc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;functional&lt;/span&gt; &lt;span class="n"&gt;GGA&lt;/span&gt;
&lt;span class="n"&gt;xc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;authors&lt;/span&gt; &lt;span class="n"&gt;PBE&lt;/span&gt;
&lt;span class="n"&gt;PAO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BasisSize&lt;/span&gt; &lt;span class="n"&gt;SZ&lt;/span&gt;
&lt;span class="n"&gt;MeshCutoff&lt;/span&gt; &lt;span class="mf"&gt;300.0&lt;/span&gt; &lt;span class="n"&gt;Ry&lt;/span&gt;
&lt;span class="n"&gt;SystemName&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
&lt;span class="n"&gt;SystemLabel&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
&lt;span class="n"&gt;UseSaveData&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;SolutionMethod&lt;/span&gt;  &lt;span class="n"&gt;qetsc&lt;/span&gt; &lt;span class="cp"&gt;#slepc0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;older&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;
&lt;span class="cp"&gt;#BlockSize 23 #Do not set for qetsc&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# System info&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;ChemicalSpeciesLabel&lt;/span&gt;
  &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;6&lt;/span&gt;  &lt;span class="n"&gt;C&lt;/span&gt;
  &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="n"&gt;H&lt;/span&gt;
  &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;  &lt;span class="n"&gt;N&lt;/span&gt;
  &lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;  &lt;span class="n"&gt;O&lt;/span&gt;
  &lt;span class="mi"&gt;5&lt;/span&gt;   &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;
&lt;span class="nf"&gt;%endblock&lt;/span&gt; &lt;span class="n"&gt;ChemicalSpeciesLabel&lt;/span&gt;

&lt;span class="cp"&gt;# ZM.UnitsLength Ang&lt;/span&gt;

&lt;span class="cp"&gt;# %block Zmatrix&lt;/span&gt;
&lt;span class="cp"&gt;#  cartesian&lt;/span&gt;
&lt;span class="cp"&gt;#    1  1.65    2.29    7.70   0 0 0&lt;/span&gt;
&lt;span class="cp"&gt;#    2  1.16    2.50    8.50   1 1 1&lt;/span&gt;
&lt;span class="cp"&gt;#    2  1.00    1.87    7.13   0 0 0&lt;/span&gt;
&lt;span class="cp"&gt;# %endblock Zmatrix&lt;/span&gt;

&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;LatticeVectors&lt;/span&gt;
  &lt;span class="mf"&gt;1.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="nf"&gt;%endblock&lt;/span&gt; &lt;span class="n"&gt;LatticeVectors&lt;/span&gt;

&lt;span class="n"&gt;AtomicCoordinatesFormat&lt;/span&gt; &lt;span class="n"&gt;NotScaledCartesianAng&lt;/span&gt;
&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;AtomicCoordinatesAndAtomicSpecies&lt;/span&gt;
    &lt;span class="mf"&gt;1.65&lt;/span&gt;    &lt;span class="mf"&gt;2.29&lt;/span&gt;    &lt;span class="mf"&gt;7.70&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="mf"&gt;1.16&lt;/span&gt;    &lt;span class="mf"&gt;2.50&lt;/span&gt;    &lt;span class="mf"&gt;8.50&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="mf"&gt;1.00&lt;/span&gt;    &lt;span class="mf"&gt;1.87&lt;/span&gt;    &lt;span class="mf"&gt;7.13&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="mf"&gt;5.18&lt;/span&gt;    &lt;span class="mf"&gt;9.45&lt;/span&gt;    &lt;span class="mf"&gt;8.69&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="mf"&gt;4.40&lt;/span&gt;    &lt;span class="mf"&gt;9.59&lt;/span&gt;    &lt;span class="mf"&gt;8.15&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="mf"&gt;5.13&lt;/span&gt;    &lt;span class="mf"&gt;8.53&lt;/span&gt;    &lt;span class="mf"&gt;8.95&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="mf"&gt;6.85&lt;/span&gt;    &lt;span class="mf"&gt;4.72&lt;/span&gt;    &lt;span class="mf"&gt;2.72&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="mf"&gt;6.83&lt;/span&gt;    &lt;span class="mf"&gt;4.42&lt;/span&gt;    &lt;span class="mf"&gt;1.81&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="mf"&gt;7.65&lt;/span&gt;    &lt;span class="mf"&gt;4.34&lt;/span&gt;    &lt;span class="mf"&gt;3.08&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="nf"&gt;%endblock&lt;/span&gt; &lt;span class="n"&gt;AtomicCoordinatesAndAtomicSpecies&lt;/span&gt;

&lt;span class="cp"&gt;# LatticeConstant 1. Ang&lt;/span&gt;

&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;SuperCell&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="nf"&gt;%endblock&lt;/span&gt; &lt;span class="n"&gt;SuperCell&lt;/span&gt;

&lt;span class="nf"&gt;%block&lt;/span&gt; &lt;span class="n"&gt;LatticeVectors&lt;/span&gt;
  &lt;span class="mf"&gt;1.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;
  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;0.0&lt;/span&gt;  &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="nf"&gt;%endblock&lt;/span&gt; &lt;span class="n"&gt;LatticeVectors&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;


&lt;span class="cp"&gt;# I/O options&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="n"&gt;LongOutput&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="cp"&gt;# UseTreeTimer .true.&lt;/span&gt;
&lt;span class="cp"&gt;# WriteEigenvalues True&lt;/span&gt;
&lt;span class="n"&gt;WriteCoorInital&lt;/span&gt;  &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteCoorStep&lt;/span&gt;    &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteForces&lt;/span&gt;      &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteCoorXmol&lt;/span&gt;    &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteMDXmol&lt;/span&gt;      &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteMDhistory&lt;/span&gt;   &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteDM&lt;/span&gt;          &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;WriteCoorCerius&lt;/span&gt;  &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;span class="n"&gt;WriteDM&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NetCDF&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;WriteDMHS&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NetCDF&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# SCF convergence options&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="n"&gt;MaxSCFIterations&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;SCFMustConverge&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;MixHamiltonian&lt;/span&gt;  &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;DM&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MixSCF1&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="cp"&gt;# DM.MixingWeight     0.2        # New DM amount for next SCF cycle&lt;/span&gt;
&lt;span class="cp"&gt;# DM.NumberPulay    14&lt;/span&gt;
&lt;span class="cp"&gt;# SCF.PulayMinimumHistory 12&lt;/span&gt;
&lt;span class="cp"&gt;# SCF.Pulay.Damping 0.5&lt;/span&gt;
&lt;span class="cp"&gt;# SCF.PulayDmaxRegion 20.0&lt;/span&gt;
&lt;span class="cp"&gt;# DM.NumberBroyden        3&lt;/span&gt;
&lt;span class="cp"&gt;# DM.PulayOnFile      false&lt;/span&gt;
&lt;span class="cp"&gt;# DM.NumberKick       20&lt;/span&gt;
&lt;span class="cp"&gt;# DM.KickMixingWeight 0.5&lt;/span&gt;
&lt;span class="cp"&gt;# DM.Pulay.Avoid.First.After.Kick .true.&lt;/span&gt;
&lt;span class="cp"&gt;# DM.Tolerance 0.01&lt;/span&gt;
&lt;span class="cp"&gt;# ElectronicTemperature 10000 K # [300 K]&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# MD options&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;span class="cp"&gt;# MD.TypeOfRun         cg&lt;/span&gt;
&lt;span class="cp"&gt;# MD.NumCGsteps       100&lt;/span&gt;
&lt;span class="cp"&gt;# MD.MaxCGDispl         0.15  Ang&lt;/span&gt;
&lt;span class="cp"&gt;# ZM.MaxDisplLength         0.01  Ang&lt;/span&gt;
&lt;span class="cp"&gt;# ZM.ForceTolLength     0.4 eV/Ang&lt;/span&gt;
&lt;span class="cp"&gt;# MD.MaxForceTol        0.01 eV/Ang&lt;/span&gt;
&lt;span class="cp"&gt;##################################################&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>How to install?</title><link href="/how-to-install.html" rel="alternate"></link><published>2017-02-16T13:36:10-06:00</published><updated>2017-02-16T13:36:10-06:00</updated><author><name>Murat Keceli</name></author><id>tag:None,2017-02-16:/how-to-install.html</id><summary type="html">&lt;h1&gt;How to install?&lt;/h1&gt;
&lt;p&gt;Siesta-QETSc uses the spectrum slicing eigensolver in SLEPc package.
SLEPc is an extension of PETSc. Hence, we need to install PETSc initially.
PETSc also manages the installation of other libraries that are required for
the eigensolver, i.e. MUMPS, METIS, ScaLAPACK etc.&lt;/p&gt;
&lt;h2&gt;PETSc installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https â€¦&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1&gt;How to install?&lt;/h1&gt;
&lt;p&gt;Siesta-QETSc uses the spectrum slicing eigensolver in SLEPc package.
SLEPc is an extension of PETSc. Hence, we need to install PETSc initially.
PETSc also manages the installation of other libraries that are required for
the eigensolver, i.e. MUMPS, METIS, ScaLAPACK etc.&lt;/p&gt;
&lt;h2&gt;PETSc installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/petsc/petsc
cd petsc
export PETSC_DIR=$PWD
export PETSC_ARCH=arch-gcc
git checkout c616f458c34eeb4fc62c # Temporary solution to avoid recent changes in PETSc
./configure  --download-mpich --download-fblaslapack --download-scalapack --download-mumps --download-metis --download-ptscotch --download-scalapack --with-shared-libraries=0 --with-debugging=0
make all test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For more information on PETSc installation check https://www.mcs.anl.gov/petsc/documentation/installation.html
For better performance use vendor specific libraries, especially for blas and lapack.
However, we do not recommend MKL, since we encountered some problems with older versions. (More info needed)&lt;/p&gt;
&lt;h2&gt;SLEPc installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/keceli/slepcfork
cd slepcfork
export SLEPC_DIR=$PWD
./configure
make all test
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Siesta installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/keceli/siesta-qetsc.git
cd siesta-qetsc
export SIESTA_DIR=&lt;span class="nv"&gt;$PWD&lt;/span&gt;
mkdir &lt;span class="nv"&gt;$PETSC_ARCH&lt;/span&gt;
cd &lt;span class="nv"&gt;$PETSC_ARCH&lt;/span&gt;
sh ../Src/obj_setup.sh
../Src/configure --enable-mpi CC=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/bin/mpicc FC=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/bin/mpif90 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Edit &lt;code&gt;arch.make&lt;/code&gt; file to change LIBS and FFLAGS as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LIBS=&amp;quot;&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_EPS_LIB&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt; $(SCALAPACK_LIBS) $(BLACS_LIBS) $(LAPACK_LIBS) $(BLAS_LIBS) $(NETCDF_LIBS)&amp;quot; 
FFLAGS=-g -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Check arch.make file to make sure LIBS includes ${SLEPC_EPS_LIB}.&lt;/p&gt;
&lt;h1&gt;How to run?&lt;/h1&gt;
&lt;p&gt;Create input file as described in Siesta manual, or see examples in Tests or Examples folder
Currently, you can only use INPUT_DEBUG as your input file due to a problem with fdf and petsc.
Once you have INPUT_DEBUG file ready, you can simply use&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mpiexec -n 2 $SIESTA_DIR/$PETSC_ARCH/siesta
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to run siesta without specifying the input file, since siesta will read INPUT_DEBUG
as the default input file.&lt;/p&gt;
&lt;h2&gt;How to use QETSc solver?&lt;/h2&gt;
&lt;p&gt;You have to set &lt;code&gt;SolutionMethod&lt;/code&gt; to &lt;code&gt;qetsc&lt;/code&gt; in INPUT_DEBUG, i.e.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SolutionMethod  qetsc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run with the following runtime options&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mpiexec -np 2 `siesta` -options_file options.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;options.txt&lt;/code&gt; file contains command line options that can be set at run time for SIESTA-QETSc runs.
Here is a sample &lt;code&gt;options.txt&lt;/code&gt; file with explanations on their usage.
You can also find options.txt file and sample siesta input files in &lt;code&gt;$SIESTA_DIR/Tests/qetsc&lt;/code&gt;
directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# SLEPc eigensolver options&lt;/span&gt;
&lt;span class="c1"&gt;# -eps_interval a,b: where a and b are real numbers setting the global interval [a,b] for the eigenvalue search.&lt;/span&gt;
-eps_interval -3.0,1.0
&lt;span class="c1"&gt;# -eps_krylovschur_partitions n: where n is an interger setting the number of bins (slices) for dividing the global interval&lt;/span&gt;
&lt;span class="c1"&gt;# n should evenly divide the number of processors. This number has an impact on the performance.&lt;/span&gt;
&lt;span class="c1"&gt;# In general it can be set equal to the number of processors. i.e. for `mpirun -np 16 siesta -options_file options.txt`&lt;/span&gt;
-eps_krylovschur_partitions &lt;span class="m"&gt;16&lt;/span&gt;

&lt;span class="c1"&gt;# PETSc options&lt;/span&gt;
-mat_type mpisbaij &lt;span class="c1"&gt;# use a sym. matrix, reducing memory footprint&lt;/span&gt;
-log_view &lt;span class="c1"&gt;# Prints out a very useful profile log&lt;/span&gt;
-memory_view &lt;span class="c1"&gt;# Prints out memory usage for petsc operations. (On some systems, it doesn&amp;#39;t work)&lt;/span&gt;
-mat_mumps_icntl_7 &lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="c1"&gt;# Sets Metis for ordering rows/columns (Serial)&lt;/span&gt;

&lt;span class="c1"&gt;# QETSc options&lt;/span&gt;

&lt;span class="c1"&gt;# Below are the options set as default, so the user does not need to change any&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptbin 7 # Sets the binning algorithm, use 0 for uniform slicing, 7 is the best nonuniform slicing&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptdensity 0 # Sets the method for density matrix calculation&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptinertia 1 # Perform inertia calculation until eigenvalues starts to converge&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptwrite 1 # Write eigenvalues in log file, use 2 to write matrices to disk&lt;/span&gt;
&lt;span class="c1"&gt;#-roptbuffer 0.1 # Buffer value for end points of the interval&lt;/span&gt;
&lt;span class="c1"&gt;#-roptdiff 0.02 # Convergence threshold for eigenvalue, to stop inertia calculations&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Troubleshooting&lt;/h2&gt;
&lt;h3&gt;Missing eigenvalues&lt;/h3&gt;
&lt;p&gt;If there are missing eigenvalues, i.e SIESTA-QETSc reports:
&lt;code&gt;Not enough eigenvalues&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can try any or all of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increase global interval for eigenvalue range,
&lt;code&gt;-eps_interval -15,5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Increase roptbuffer
&lt;code&gt;-roptbuffer 0.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Increase ioptinertia
&lt;code&gt;-ioptinertia 100&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Decrease roptdiff
&lt;code&gt;-roptdiff 0.0001&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Factorization problem&lt;/h3&gt;
&lt;p&gt;If you get an error from MUMPS during factorization, i.e.
&lt;code&gt;Error reported by MUMPS in numerical factorization phase: INFOG(1)=-9,&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can try any or all of the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decrease number of bins (slices, partitions) using  &lt;code&gt;-eps_krylovschur_partitions&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change to parallel symbolic factorization&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-mat_mumps_icntl_28 2
-mat_mumps_icntl_29 1 # ptscotch
#-mat_mumps_icntl_29 2 # parmetis
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Set available memory per core in MB:
&lt;code&gt;-mat_mumps_icntl_23 2500&lt;/code&gt;
If none of these help, the problem might be insufficient memory for the machine you use, either 
decrease the problem size or switch to a computer with larger memory per core.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;More configure options from Siesta manual:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;-DMPI_TIMING: to obtain the accounting of MPI communication times in parallel executions&lt;/li&gt;
&lt;li&gt;-DGRID_SP: to use single-precision for all the grid
magnitudes, including the orbitals array and charge densities and potentials. This will
cause some numerical dierences and will have a negligible eect on memory consumption,
since the orbitals array is the main user of memory on the grid, and it is single-precision
by default. This setting will recover the default behavior of previous versions of Siesta.&lt;/li&gt;
&lt;li&gt;-DGRID_DP: to use double-precision for all the grid
magnitudes, including the orbitals array. This will significantly increase the memory used
for large problems, with negligible diferences in accuracy.&lt;/li&gt;
&lt;li&gt;-DBROYDEN_DP: to use double-precision arrays for the
Broyden historical data sets. (Remember that the Broyden mixing for SCF convergence
acceleration is an experimental feature.)&lt;/li&gt;
&lt;li&gt;-DON_DP: to use double-precision for all the arrays
in the O(N) routines.&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Benchmark Results for SIESTA-QETSc</title><link href="/benchmark-results-for-siesta-qetsc.html" rel="alternate"></link><published>2017-01-06T16:08:42-06:00</published><updated>2017-01-06T16:08:42-06:00</updated><author><name>Murat Keceli</name></author><id>tag:None,2017-01-06:/benchmark-results-for-siesta-qetsc.html</id><summary type="html">&lt;h1&gt;Strong scaling&lt;/h1&gt;
&lt;p&gt;Strong scaling plot for the average timing of an SCF iteration
for LDA energy and gradient calculation.
Time obtained with QETSc and ScaLAPACK are shown with
filled blue circles and red squares, respectively.
Error bars show the spread of SCF iterations with QETSc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Polyethlyene with 64 unit cells â€¦&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Strong scaling&lt;/h1&gt;
&lt;p&gt;Strong scaling plot for the average timing of an SCF iteration
for LDA energy and gradient calculation.
Time obtained with QETSc and ScaLAPACK are shown with
filled blue circles and red squares, respectively.
Error bars show the spread of SCF iterations with QETSc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Polyethlyene with 64 unit cells, 384 atoms, 2944 orbitals.
&lt;img src="/images/p64_strong_vesta-1.png" width="600"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Polyethlyene with 256 unit cells, 1536 atoms, 11776 orbitals.
&lt;img src="/images/p256_strong_vesta-1.png" width="600"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;128 water clusters, 384 atoms, 2944 orbitals.
&lt;img src="/images/w128_strong_vesta-1.png" width="600"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;512 water clusters, 1536 atoms, 11776 orbitals.
&lt;img src="/images/w512_strong_vesta-1.png" width="600"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Eigenvalues and slices&lt;/h1&gt;
&lt;p&gt;Eigenvalues and slices are shown for the molecular dynamics
simulation of Boron nitride monolayer with 512 atoms.
Only 3 MD steps are shown.
&lt;img src="/images/b16cg_bins-1.png" width="800"&gt;&lt;/p&gt;</content></entry><entry><title>QETSc</title><link href="/qetsc.html" rel="alternate"></link><published>2017-01-03T01:54:00-06:00</published><updated>2017-01-03T01:54:00-06:00</updated><author><name>Murat Keceli</name></author><id>tag:None,2017-01-03:/qetsc.html</id><summary type="html">&lt;h1&gt;SIESTA-QETSc&lt;/h1&gt;
&lt;p&gt;Siesta-QETSc is a branch of Siesta (see README_SIESTA) distinguished by
a state-of-the-art parallel sparse eigensolver, which significantly improves the
performance of the code for large calculations i.e. more than a few hundred atoms
or a few thousand basis functions.&lt;/p&gt;
&lt;p&gt;For more information on the eigensolver you can check â€¦&lt;/p&gt;</summary><content type="html">&lt;h1&gt;SIESTA-QETSc&lt;/h1&gt;
&lt;p&gt;Siesta-QETSc is a branch of Siesta (see README_SIESTA) distinguished by
a state-of-the-art parallel sparse eigensolver, which significantly improves the
performance of the code for large calculations i.e. more than a few hundred atoms
or a few thousand basis functions.&lt;/p&gt;
&lt;p&gt;For more information on the eigensolver you can check the following papers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Zhang, H.; Smith, B.; Sternberg, M.; Zapol, P. SIPs: Shift-and-Invert Parallel Spectral Transformations. ACM Trans. Math. Softw. 2007, 33, 9â€“es.&lt;/li&gt;
&lt;li&gt;Campos, C.; RomÃ¡n, J. E. Strategies for Spectrum Slicing Based on Restarted Lanczos Methods. Numer. Algorithms 2012, 60, 279â€“295.&lt;/li&gt;
&lt;li&gt;KeÃ§eli, M.; Zhang, H.; Zapol, P.; Dixon, D. A.; Wagner, A. F. Shift-and-Invert Parallel Spectral Transformation Eigensolver: Massively Parallel Performance for Density-Functional Based Tight-Binding. J. Comput. Chem. 2016, 37, 448â€“459.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;How to install?&lt;/h1&gt;
&lt;p&gt;Siesta-QETSc uses the spectrum slicing eigensolver in SLEPc package.
SLEPc is an extension of PETSc. Hence, we need to install PETSc initially.
PETSc also manages the installation of other libraries that are required for
the eigensolver, i.e. MUMPS, METIS, ScaLAPACK etc.&lt;/p&gt;
&lt;h2&gt;PETSc installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/petsc/petsc
cd petsc
export PETSC_DIR=$PWD
export PETSC_ARCH=arch-gcc
git checkout c616f458c34eeb4fc62c # Temporary solution to avoid recent changes in PETSc
./configure  --download-mpich --download-fblaslapack --download-scalapack --download-mumps --download-metis --download-ptscotch --download-scalapack --with-shared-libraries=0 --with-debugging=0
make all test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For more information on PETSc installation check https://www.mcs.anl.gov/petsc/documentation/installation.html
For better performance use vendor specific libraries, especially for blas and lapack.
However, we do not recommend MKL, since we encountered some problems with older versions. (More info needed)&lt;/p&gt;
&lt;h2&gt;SLEPc installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/keceli/slepcfork
cd slepcfork
export SLEPC_DIR=$PWD
./configure
make all test
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Siesta installation:&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://bitbucket.org/keceli/siesta-qetsc.git
cd siesta-qetsc
mkdir &lt;span class="nv"&gt;$PETSC_ARCH&lt;/span&gt;
cd &lt;span class="nv"&gt;$PETSC_ARCH&lt;/span&gt;
sh ../Src/obj_setup.sh
../Src/configure --enable-mpi CC=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/bin/mpicc FC=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/bin/mpif90 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Edit &lt;code&gt;arch.make&lt;/code&gt; file to change LIBS and FFLAGS as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LIBS=&amp;quot;&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_EPS_LIB&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt; $(SCALAPACK_LIBS) $(BLACS_LIBS) $(LAPACK_LIBS) $(BLAS_LIBS) $(NETCDF_LIBS)&amp;quot; 
FFLAGS=-g -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PETSC_ARCH&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include -I&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;SLEPC_DIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;/include
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Check arch.make file to make sure LIBS includes ${SLEPC_EPS_LIB}.&lt;/p&gt;
&lt;h1&gt;How to run?&lt;/h1&gt;
&lt;p&gt;Create input file as described in Siesta manual, or see examples in Tests or Examples folder
Currently, you can only use INPUT_DEBUG as your input file due to a problem with fdf and petsc.
Once you have INPUT_DEBUG file ready, you can simply use&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mpiexec -n 2 siesta
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to run siesta without specifying the input file, since siesta will read INPUT_DEBUG
as the default input file.&lt;/p&gt;
&lt;h2&gt;How to use QETSc solver?&lt;/h2&gt;
&lt;p&gt;You have to set SolutionMethod to qetsc in INPUT_DEBUG, i.e.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SolutionMethod  qetsc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run with the following runtime options&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mpiexec -np 2 `siesta` -options_file options.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;options.txt&lt;/code&gt; file contains command line options that can be set at run time for SIESTA-QETSc runs.
Here is a sample &lt;code&gt;options.txt&lt;/code&gt; file with explanations on their usage.
You can also fine options.txt file and sample siesta input files in &lt;code&gt;SIESTA_DIRECTORY/Tests/qetsc&lt;/code&gt;
directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# SLEPc eigensolver options&lt;/span&gt;
&lt;span class="c1"&gt;# -eps_interval a,b: where a and b are real numbers setting the global interval [a,b] for the eigenvalue search.&lt;/span&gt;
-eps_interval -3.0,1.0
&lt;span class="c1"&gt;# -eps_krylovschur_partitions n: where n is an interger setting the number of bins (slices) for dividing the global interval&lt;/span&gt;
&lt;span class="c1"&gt;# n should evenly divide the number of processors. This number has an impact on the performance.&lt;/span&gt;
&lt;span class="c1"&gt;# In general it can be set equal to the number of processors. i.e. for `mpirun -np 16 siesta -options_file options.txt`&lt;/span&gt;
-eps_krylovschur_partitions &lt;span class="m"&gt;16&lt;/span&gt;

&lt;span class="c1"&gt;# PETSc options&lt;/span&gt;
-mat_type mpisbaij &lt;span class="c1"&gt;# use a sym. matrix, reducing memory footprint&lt;/span&gt;
-log_view &lt;span class="c1"&gt;# Prints out a very useful profile log&lt;/span&gt;
-memory_view &lt;span class="c1"&gt;# Prints out memory usage for petsc operations. (On some systems, it doesn&amp;#39;t work)&lt;/span&gt;
-mat_mumps_icntl_7 &lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="c1"&gt;# Sets Metis for ordering rows/columns (Serial)&lt;/span&gt;

&lt;span class="c1"&gt;# QETSc options&lt;/span&gt;

&lt;span class="c1"&gt;# Below are the options set as default, so the user does not need to change any&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptbin 7 # Sets the binning algorithm, use 0 for uniform slicing, 7 is the best nonuniform slicing&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptdensity 0 # Sets the method for density matrix calculation&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptinertia 1 # Perform inertia calculation until eigenvalues starts to converge&lt;/span&gt;
&lt;span class="c1"&gt;#-ioptwrite 1 # Write eigenvalues in log file, use 2 to write matrices to disk&lt;/span&gt;
&lt;span class="c1"&gt;#-roptbuffer 0.1 # Buffer value for end points of the interval&lt;/span&gt;
&lt;span class="c1"&gt;#-roptdiff 0.02 # Convergence threshold for eigenvalue, to stop inertia calculations&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Troubleshooting&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;If there are missing eigenvalues, i.e SIESTA-QETSc reports:
&lt;code&gt;Not enough eigenvalues&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can try any or all of the following:
    1. Increase global interval for eigenvalue range,
-eps_interval -15,5
    2. Increase roptbuffer
-roptbuffer 0.5
    3. Increase ioptinertia
-ioptinertia 100
    4. Decrease roptdiff
-roptdiff 0.0001&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If you get an error from MUMPS during factorization, i.e. 
&lt;code&gt;Error reported by MUMPS in numerical factorization phase: INFOG(1)=-9,&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can try any or all of the following.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1. Decrease number of bins (slices, partitions) using  -eps_krylovschur_partitions
2. Change to parallel symbolic factorization
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-mat_mumps_icntl_28 2
-mat_mumps_icntl_29 1 # ptscotch
#-mat_mumps_icntl_29 2 # parmetis
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;3. Set available memory per core in MB:
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;-mat_mumps_icntl_23 2500&lt;/code&gt;
If none of these help, the problem might be too big for the machine you use, either change the problem or the computer.&lt;/p&gt;
&lt;h2&gt;More configure options from Siesta manual:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;-DMPI_TIMING: to obtain the accounting of MPI communication times in parallel executions&lt;/li&gt;
&lt;li&gt;-DGRID_SP: to the DEFS variable in arch.make to use single-precision for all the grid
magnitudes, including the orbitals array and charge densities and potentials. This will
cause some numerical dierences and will have a negligible eect on memory consumption,
since the orbitals array is the main user of memory on the grid, and it is single-precision
by default. This setting will recover the default behavior of previous versions of Siesta.&lt;/li&gt;
&lt;li&gt;-DGRID_DP: to the DEFS variable in arch.make to use double-precision for all the grid
magnitudes, including the orbitals array. This will significantly increase the memory used
for large problems, with negligible diferences in accuracy.&lt;/li&gt;
&lt;li&gt;-DBROYDEN_DP: to the DEFS variable in arch.make to use double-precision arrays for the
Broyden historical data sets. (Remember that the Broyden mixing for SCF convergence
acceleration is an experimental feature.)&lt;/li&gt;
&lt;li&gt;-DON_DP: to the DEFS variable in arch.make to use double-precision for all the arrays
in the O(N) routines.&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>